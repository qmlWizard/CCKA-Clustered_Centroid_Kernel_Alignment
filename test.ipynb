{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Custom Libraries\n",
    "from utils.model import Qkernel\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.agent import TrainModel\n",
    "\n",
    "# Backend Configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Configs\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data = np.load('checkerboard_dataset.npy', allow_pickle=True).item()\n",
    "x_train, x_test, y_train, y_test = data['x_train'], data['x_test'], data['y_train'], data['y_test']\n",
    "\n",
    "training_data = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
    "testing_data = torch.tensor(x_test, dtype=torch.float32, requires_grad=True)\n",
    "training_labels = torch.tensor(y_train, dtype=torch.int)\n",
    "testing_labels = torch.tensor(y_test, dtype=torch.int)\n",
    "\n",
    "kernel = Qkernel(   \n",
    "                        device = config['qkernel']['device'], \n",
    "                        n_qubits = 4, \n",
    "                        trainable = True, \n",
    "                        input_scaling = True, \n",
    "                        data_reuploading = True, \n",
    "                        ansatz = 'embedding_paper', \n",
    "                        ansatz_layers = 5\n",
    "                    )\n",
    "    \n",
    "agent = TrainModel(\n",
    "                        kernel=kernel,\n",
    "                        training_data=training_data,\n",
    "                        training_labels=training_labels,\n",
    "                        testing_data=testing_data,\n",
    "                        testing_labels=testing_labels,\n",
    "                        optimizer= 'adam',\n",
    "                        lr= 0.1,\n",
    "                        epochs = 200,\n",
    "                        train_method= 'ccka',\n",
    "                        target_accuracy=0.95,\n",
    "                        get_alignment_every=10,  \n",
    "                        validate_every_epoch=None, \n",
    "                        base_path='.',\n",
    "                        lambda_kao=0.01,\n",
    "                        lambda_co=0.1,\n",
    "                        clusters=4\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for main centroids and class centroids\n",
    "main_centroids = []\n",
    "class_centroids = []\n",
    "main_centroid_labels = []\n",
    "class_centroid_labels = []\n",
    "\n",
    "for c in [1, -1]:\n",
    "    class_data = training_data[training_labels == c]\n",
    "    # Calculate the main centroid and add it to main_centroids\n",
    "    main_centroid = torch.mean(class_data, axis = 0).requires_grad_()  # Shape [1, feature_dim]\n",
    "    main_centroids.append(main_centroid)\n",
    "    main_centroid_labels.append(c)\n",
    "    \n",
    "            # Calculate centroids for each cluster in the class and stack them into a single tensor\n",
    "    #class_centroids.append([np.mean(cluster.tolist(), axis=0).tolist() for cluster in np.array_split(class_data, 4)])\n",
    "    #class_centroid_labels.append([c] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data.detach().numpy()\n",
    "data_labels = training_labels.detach().numpy()\n",
    "\n",
    "\n",
    "for c in [1, -1]:\n",
    "    cdata = data[data_labels == c]\n",
    "    mc = [np.mean(cdata, axis=0)]\n",
    "    sub_centroids = [np.mean(cluster, axis=0) for cluster in np.array_split(cdata, 4)]\n",
    "    class_centroids = mc + sub_centroids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.47623625, 0.5011184 ], dtype=float32),\n",
       " array([0.37070206, 0.6296703 ], dtype=float32),\n",
       " array([0.36902234, 0.50664234], dtype=float32),\n",
       " array([0.64839965, 0.39306217], dtype=float32),\n",
       " array([0.5303492 , 0.46642599], dtype=float32)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_main_centroids = np.stack(main_centroid)\n",
    "_main_centroids = torch.tensor(_main_centroids, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2706, 0.4972], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_class_centroids = np.vstack(class_centroids)\n",
    "_class_centroids = torch.tensor(_class_centroids, requires_grad=True)\n",
    "_class_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_optimizer = optim.SGD([\n",
    "            {'params': _main_centroids, 'lr': 0.1},\n",
    "            {'params': _class_centroids, 'lr': 0.1},\n",
    "            #{'params': self._class_centroids, 'lr': self._lr},\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"\"\"\n",
    "    def _get_centroids(self, data, data_labels):\n",
    "        for c in self._n_classes:\n",
    "            class_data = data[data_labels == c]\n",
    "            main_centroid = torch.mean(class_data, dim=0)\n",
    "            self._main_centroids.append(main_centroid.requires_grad_())\n",
    "            self._main_centroids_labels.append(c)\n",
    "            class_centroids = [torch.mean(cluster, dim=0) for cluster in torch.chunk(class_data, self._clusters)]\n",
    "            self._class_centroids.append([centroid.requires_grad_() for centroid in class_centroids])\n",
    "            self._class_centroid_labels.append([c] * self._clusters)\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_centroids(self, data, data_labels):\n",
    "        # Initialize empty lists for main centroids and class centroids\n",
    "        main_centroids = []\n",
    "        class_centroids = []\n",
    "        main_centroid_labels = []\n",
    "        class_centroid_labels = []\n",
    "\n",
    "        for c in self._n_classes:\n",
    "            class_data = np.array(data[data_labels == c].detach())\n",
    "            # Calculate the main centroid and add it to main_centroids\n",
    "            main_centroid = np.mean(class_data, axis = 0)  # Shape [1, feature_dim]\n",
    "            main_centroids.append(main_centroid.tolist())\n",
    "            main_centroid_labels.append(c)\n",
    "    \n",
    "            # Calculate centroids for each cluster in the class and stack them into a single tensor\n",
    "            class_centroids.append([np.mean(cluster.tolist(), axis=0).tolist() for cluster in np.array_split(class_data, self._clusters)])\n",
    "            class_centroid_labels.append([c] * self._clusters)\n",
    "\n",
    "        self._main_centroids = torch.tensor(main_centroids, requires_grad=True)\n",
    "        self._main_centroids_labels = torch.tensor(main_centroid_labels)\n",
    "        self._class_centroids = torch.tensor(class_centroids, requires_grad=True)\n",
    "        self._class_centroid_labels = torch.tensor(class_centroid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  _class = epoch % len(self._n_classes)\n",
    "                class_centroids = torch.tensor(self._class_centroids[_class], requires_grad=True)\n",
    "                class_labels = self._class_centroid_labels[_class]\n",
    "                main_centroid = torch.tensor(self._main_centroids[_class], requires_grad=True)\n",
    "\n",
    "                x_0 = main_centroid.repeat(self._clusters, 1)\n",
    "                x_1 = class_centroids\n",
    "                \n",
    "                K = self._kernel(x_0, x_1).to(torch.float32)\n",
    "\n",
    "                loss = self._centroid_loss(K = K, Y=class_labels, centroid=main_centroid, cl=_class + 1)\n",
    "                loss = loss.mean()\n",
    "                loss.backward()\n",
    "\n",
    "                for param in self._kernel.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        print(f\"Kernel Gradient: {param.grad}\")\n",
    "                for param in [self._main_centroids[_class], K]:\n",
    "                    print(f\"Centroid Gradient: {param.grad}\")\n",
    "            \n",
    "                optimizer.step()\n",
    "                print(f\"Epoch {epoch + 1}th, Kernel Loss: {loss}\" )\n",
    "\n",
    "                self._loss_arr.append(loss.item())\n",
    "                self._per_epoch_executions += x_0.shape[0]\n",
    "                \"\"\"\n",
    "                # Kao loss\n",
    "                loss_kao = -self._loss_kao(class_centroids, class_labels, self._main_centroids[_class])\n",
    "                loss_kao.backward(retain_graph=True)\n",
    "                optimizer.step() \n",
    "\n",
    "                # Co loss\n",
    "                self._centroid_optimizer.zero_grad()\n",
    "                loss_co = -self.loss_co(class_centroids, class_labels, self._main_centroids[_class], _class + 1)\n",
    "                loss_co.backward(retain_graph=True)\n",
    "                self._centroid_optimizer.step()\n",
    "                \n",
    "                \n",
    "\n",
    "                loss_kao, loss_co = self._centroid_loss(class_centroids, class_labels, self._main_centroids[_class], _class + 1)\n",
    "                loss_kao.backward(retain_graph=True)\n",
    "                optimizer.step() \n",
    "                \n",
    "                loss_co.backward(retain_graph=True)\n",
    "                self._centroid_optimizer.step()\n",
    "\n",
    "                self._per_epoch_executions += self._kernel._circuit_executions\n",
    "                print(self._per_epoch_executions)\n",
    "                print(f\"Epoch {epoch + 1}th, Kernel Loss: {loss}\" )\n",
    "                \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgreedy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
